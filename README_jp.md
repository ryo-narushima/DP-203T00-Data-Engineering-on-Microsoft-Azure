# DP-203T00: Data Engineering on Azure

DP-203: Data Engineering on Azure のコースへようこそ。このコースに対応するため、コースで使用するAzureのサービスを最新のものにするために、コース内容の更新を行う必要があります。 私たちは、Azure プラットフォームの変更に伴い、コンテンツを最新の状態に保つために、コース作成者と MCT の間でオープンな貢献を可能にするために、ラボの指示とラボファイルを GitHub で公開しています。

- **Are you a MCT?** - Have a look at our [GitHub User Guide for MCTs](https://microsoftlearning.github.io/MCT-User-Guide/).
                                                                       
##  公開されているMOCファイルと比較して、どのように使用すればいいのでしょうか？

- コース内容を教えるには、これまで通りインストラクターハンドブックとパワーポイントが主な資料となる予定です。

- GitHub上のこれらのファイルは、学生向けハンドブックとともに使用するように設計されていますが、MCTとコース作成者が最新のラボファイルのための共有ソースを持つことができるように、中央リポジトリとしてGitHubにあるされています。

- 各モジュールの実験指示は、/Instructions/Labs フォルダにあります。この場所内の各サブフォルダは、各モジュールを参照してください。例えば、Lab01はmodule01に関連しているなどです。README.md ファイルが各フォルダにあり、学生が従うべき実験指示が記載されています。

- 受講者は、配信のたびに GitHub をチェックして、最新の Azure サービスに対応した変更がないか確認し、配信用の最新ファイルを取得することが推奨されます。

- このラボの指示で表示される一部の画像は、必ずしもこのコースで使用するラボ環境の状態を反映していないことに注意してください。例えば、データレイクでファイルをブラウズしているとき、あなたの環境には存在しないかもしれないadiitionalフォルダが画像に表示されるかもしれません。これは意図的なものであり、ラボの指示はそのまま機能します。

## 学生ハンドブックの変更についてはどうですか？

- 四半期ごとに学生便覧を見直し、必要に応じて通常のMOCリリースルートで更新します。

## How do I contribute?

- MCTは誰でもGitHub reproでコードやコンテンツに問題を提出することができます、マイクロソフトとコースの著者は、必要に応じてコンテンツやラボコードの変更をトリアージして含めます。

## Classroom Materials

MCTs とパートナーはこれらの材料にアクセスし、順番に、学生にそれらを個別に提供することが強く推奨されます。 進行中のクラスの一部としてLabステップにアクセスするためにGitHubに直接学生を指摘することは、コースの一部としてさらに別のUIにアクセスする必要があり、学生のための混乱を助長する。なぜ、Labの手順が別に用意されているのかを学生に説明することで、常に変化するクラウドベースのインターフェースとプラットフォームの性質を強調することができます。GitHub上のファイルにアクセスするためのMicrosoft LearningのサポートとGitHubサイトのナビゲーションのサポートは、このコースを教えるMCTにのみ限定されています。

## Lab overview

以下は、各モジュールのラボの目的の概要です。

### ラボ 1 - データエンジニアリングワークロードのための計算機とストレージのオプションの検討

このラボでは、データレイクを構造化する方法と、探索、ストリーミング、およびバッチワークロード用にファイルを最適化する方法を学びます。バッチ処理とストリーム処理でファイルを変換しながら、データレイクをデータの精緻化レベルに整理する方法を学びます。また、Azure Synapse AnalyticsでApache Sparkを使用した作業も体験します。 CSV、JSON、Parquetファイルなどのデータセットにインデックスを作成し、HyperspaceやMSSParkUtilsなどのSparkライブラリを使用してクエリやワークロードを高速化する方法について学びます。

### ラボ 2 - サービング層の設計と実装

このラボでは、分析ワークロードを最適化するために、最新のデータウェアハウスでデータストアを設計および実装する方法を学びます。ファクトデータとディメンションデータを格納する多次元スキーマを設計する方法を学びます。次に、Azure Data Factory からの増分データロードにより、徐々に変化するディメンションを入力する方法を学びます。

### ラボ 3 - ソースファイルに関するデータエンジニアリングの考慮事項

このラボでは、講師の指示のもと、一人またはグループで20分間、以下に紹介する内容に目を通していただきます。その後、質問に答え、要件に基づいた結果を教室で発表してください。

### ラボ 4 - Azure Synapse AnalyticsのサーバーレスSQLプールを使用したインタラクティブなクエリの実行

このラボでは、Azure Synapse AnalyticsのサーバーレスSQLプールで実行されるT-SQLステートメントを通じて、データレイクや外部ファイルソースに保存されているファイルの操作方法を学習します。データレイクに保存されているParquetファイルや、外部データストアに保存されているCSVファイルにクエリを実行します。次に、Azure Active Directoryのセキュリティグループを作成し、ロールベースアクセス制御（RBAC）とアクセス制御リスト（ACL）を使用してデータレイク内のファイルへのアクセスを強制します。

### ラボ 5 - Apache Spark を使用して、データを探索し、変換し、データウェアハウスにロードする。

このラボでは、データレイクに保存されたデータを探索し、データを変換し、リレーショナルデータストアにデータをロードする方法を学びます。Parquet と JSON ファイルを調査し、階層構造を持つ JSON ファイルをクエリーして変換するテクニックを使用します。次に、Apache Spark を使用して、データをデータウェアハウスにロードし、データレイクの Parquet データと専用の SQL プールのデータを結合します。

### ラボ6：Azure Databricksにおけるデータの探索と変換

このラボでは、Apache Spark DataFrame のさまざまなメソッドを使用して、Azure Databricks でデータを探索および変換する方法について学びます。標準的な DataFrame メソッドを実行して、データを探索および変換する方法を学習します。また、重複データの削除、日付/時刻値の操作、列名の変更、データの集約など、より高度なタスクの実行方法についても学習します。選択した取り込み技術をプロビジョニングし、これをStream Analyticsと統合して、ストリーミングデータで動作するソリューションを作成します。

### ラボ7：データウェアハウスへのデータの取り込みとロード

このラボでは、T-SQLスクリプトとSynapse Analytics統合パイプラインを使用して、データウェアハウスにデータを取り込む方法を学習します。T-SQLを使用して、PolyBaseとCOPYを使用してSynapse専用のSQLプールにデータをロードする方法を学びます。また、ペタバイト規模のデータを取り込むために、Azure Synapseパイプラインのコピーアクティビティとワークロード管理の使用方法についても学習します。

### ラボ8 - Azure Data FactoryまたはAzure Synapseパイプラインを使用してデータを変換する

このラボでは、複数のデータソースから取り込み、マッピングデータフローとノートブックを使用してデータを変換し、1つまたは複数のデータシンクにデータを移動するためのデータ統合パイプラインを構築する方法を学習します。

### ラボ 9 - Azure Data Factory または Azure Synapse パイプラインを使用してノートブックからデータを統合する。

このラボでは、ユーザーのアクティビティと過去12ヶ月間の購入履歴を照会するためのノートブックを作成します。次に、新しいノートブックアクティビティを使用してパイプラインにノートブックを追加し、オーケストレーションプロセスの一部としてマッピングデータフローの後にこのノートブックを実行します。これを設定しながら、コントロールフローに動的なコンテンツを追加するためのパラメータを実装し、パラメータの使用方法を検証します。

### ラボ 10 - Azure Synapse の専用 SQL プールでクエリパフォーマンスを最適化する

このラボでは、Azure Synapse Analyticsで専用SQLプールを使用する際に、データの保存と処理を最適化するための戦略を学習します。ウィンドウやHyperLogLog関数などの開発者機能の使用方法、データロードのベストプラクティス、クエリパフォーマンスの最適化および向上について学びます。

### ラボ11：データウェアハウスストレージの解析と最適化

このラボでは、Azure Synapse 専用 SQL プールのデータ ストレージを分析および最適化する方法について学習します。テーブル スペースの使用状況とカラム ストア ストレージの詳細を理解するためのテクニックを学びます。次に、異なるデータ型を使用する同一のテーブル間のストレージ要件を比較する方法を学びます。最後に、複雑なクエリの代わりにマテリアライズドビューを実行した場合の影響を観察し、削除操作を最適化することによって大規模なログを回避する方法を学習します。

### ラボ 12 - Azure Synapse Link によるハイブリッドトランザクション分析処理 (HTAP) のサポート

このラボでは、Azure Synapse LinkによってAzure Cosmos DBアカウントとSynapseワークスペースをシームレスに接続する方法について学びます。Synapse Link を有効にして設定する方法と、Apache Spark と SQL Serverless を使用して Azure Cosmos DB 分析ストアにクエリを実行する方法を理解します。

### ラボ13 - Azure Synapse Analyticsを使用したエンドツーエンドのセキュリティ

このラボでは、Synapse Analyticsのワークスペースとそれをサポートするインフラストラクチャを保護する方法を学習します。SQL Active Directory Admin の観察、IP ファイアウォール ルールの管理、Azure Key Vault による秘密の管理、Key Vault リンク サービスとパイプライン アクティビティによるこれらの秘密へのアクセスについて学習します。専用SQLプールを使用した、列レベルのセキュリティ、行レベルのセキュリティ、動的データマスキングの実装方法を理解することができます。

### ラボ 14 - Stream Analytics によるリアルタイム・ストリーム処理

このラボでは、Azure Stream Analyticsを使用してストリーミングデータを処理する方法を学びます。車両遠隔測定データをイベントハブに取り込み、Azure Stream Analytics のさまざまなウィンドウ機能を使って、そのデータをリアルタイムで処理します。そのデータをAzure Synapse Analyticsに出力します。最後に、Stream Analyticsのジョブをスケールしてスループットを向上させる方法について学びます。

### ラボ 15 - Event Hubs と Azure Databricks を使用したストリーム処理ソリューションの作成

このラボでは、Azure DatabricksのEvent HubsとSpark Structured Streamingを使用して、ストリーミングデータを大規模に取り込み処理する方法を学びます。Structured Streaming の主な機能と用途を学びます。また、スライディングウィンドウを実装してデータの塊を集約し、電子透かしを適用して古くなったデータを削除します。最後に、イベントハブに接続し、ストリームの読み取りと書き込みを行います。

### ラボ16 - Power BIとAzure Synapse Analyticsの統合を使用したレポートの作成

この実習では、Power BI と Azure Synapse ワークスペースを統合し、Power BI でレポートを作成する方法を学習します。Azure Synapse Studio で新しいデータ ソースと Power BI レポートを作成します。次に、マテリアライズドビューと結果セットキャッシングを使用してクエリのパフォーマンスを向上させる方法を学習します。最後に、サーバーレス SQL プールを使用してデータレイクを探索し、Power BI でそのデータに対するビジュアライゼーションを作成します。

### ラボ 17 - Azure Synapse Analytics で統合された機械学習処理を実行する

このラボでは、Azure Synapse Analytics で、統合されたエンドツーエンドの Azure Machine Learning と Azure Cognitive Services のエクスペリエンスを探求します。Linked Service を使用して Azure Synapse Analytics ワークスペースを Azure Machine Learning ワークスペースに接続し、Spark テーブルからのデータを使用する自動化 ML 実験をトリガーする方法を学びます。また、Azure Machine LearningまたはAzure Cognitive Servicesから学習済みモデルを使用して、SQLプールテーブルのデータをリッチ化し、Power BIを使用して予測結果を提供する方法を学習します。
